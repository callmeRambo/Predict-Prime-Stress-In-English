{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import submission\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pickle\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "def get_Vocab(s):\n",
    "    return s.split(\":\")\n",
    "def s_has_pre(s):\n",
    "    pre = \"an,dis,in,ig,il,im,ir,ne,n,non,neg,un,male,mal,pseudo,mis,de\\\n",
    "    un,anti,ant,contra,contre,contro,counter,ob,oc,of,op,with,by,circum,\\\n",
    "    circu,de,en,ex,ec,es,fore,in,il,im,ir,inter,intel,intro,medi,med,mid,out,\\\n",
    "    over,post,pre,pro,sub,suc,suf,sug,sup,sur,sus,sur,trans,under,up,\\\n",
    "    ante,anti,ex,fore,mid,medi,post,pre,pri,out,over,post,pre,pro,sub,suc,suf,\\\n",
    "    sug,sum,sup,sur,sus,super,sur,trans,under,up,ante,anti,ex,fore,mid,medi,post,\\\n",
    "    pre,pri,pro,re,by,extra,hyper,out,over,sub,suc,sur,super,sur,under,vice,com,\\\n",
    "    cop,con,cor,co,syn,syl,sym,al,over,pan,ex,for,re,se,dia,per,pel,trans，ad,\\\n",
    "    ac,af,ag,an,ap,ar,as,at,ambi,bin,di,twi,tri,thir,deca,deco,dec,deci,hecto,\\\n",
    "    hect,centi,kilo,myria,mega,micro,multi,poly,hemi,demi,semi,pene,arch,auto,bene,\\\n",
    "    eu,male,mal,macro,magni,micro,aud,bio,ge,phon,tele,\\\n",
    "    ac,ad,af,ag,al,an,ap,as,at,an,ab,abs,acer,acid,acri,act,ag,acu,aer,aero,ag,agi,\\\n",
    "    ig,act,agri,agro,alb,albo,ali,allo,alter,alt,am,ami,amor,ambi,ambul,ana,ano,andr,\\\n",
    "    andro,ang,anim,ann,annu,enni,ante,anthrop,anti,ant,anti,antico,apo,ap,aph,aqu,arch,\\\n",
    "    aster,astr,auc,aug,aut,aud,audi,aur,aus,aug,auc,aut,auto,bar,be,belli,bene,bi,bine,\\\n",
    "    bibl,bibli,biblio,bio,bi,brev,cad,cap,cas,ceiv,cept,capt,cid,cip,cad,cas,calor,capit,\\\n",
    "    capt,carn,cat,cata,cath,caus,caut,cause,cuse,cus,ceas,ced,cede,ceed,cess,cent,centr,\\\n",
    "    centri,chrom,chron,cide,cis,cise,circum,cit,civ,clam,claim,clin,clud,clusclaus,co,cog,\\\n",
    "    col,coll,con,com,cor,cogn,gnos,com,con,contr,contra,counter,cord,cor,cardi,corp,cort,\\\n",
    "    cosm,cour,cur,curr,curs,crat,cracy,cre,cresc,cret,crease,crea,cred,cresc,cret,crease,\\\n",
    "    cru,crit,cur,curs,cura,cycl,cyclo,de,dec,deca,dec,dign,dei,div,dem,demo,dent,dont,derm,\\\n",
    "    di,dy,dia,dic,dict,dit,dis,dif,dit,doc,doct,domin,don,dorm,dox,duc,duct,dura,dynam,dys,\\\n",
    "    ec,eco,ecto,en,em,end,epi,equi,erg,ev,et,ex,exter,extra,extro,fa,fess,fac,fact,fec,fect,\\\n",
    "    fic,fas,fea,fall,fals,femto,fer,fic,feign,fain,fit,feat,fid,fid,fide,feder,fig,fila,fili,\\\n",
    "    fin,fix,flex,flect,flict,flu,fluc,fluv,flux,for,fore,forc,fort,form,fract,frag,frai,fuge,\\\n",
    "    fuse,gam,gastr,gastro,gen,gen,geo,germ,gest,giga,gin,gloss,glot,glu,glo,gor,grad,gress\\\n",
    "    ,gree,graph,gram,graf,grat,grav,greg,hale,heal,helio,hema,hemo,her,here,hes,hetero,hex\\\n",
    "    ,ses,sex,homo,hum,human,hydr,hydra,hydro,hyper,hypn,an,ics,ignis,in,im,in,im,il,ir,infra\\\n",
    "    ,inter,intra,intro,ty,jac,ject,join,junct,judice,jug,junct,just,juven,labor,lau,lav,lot\\\n",
    "    ,lut,lect,leg,lig,leg,levi,lex,leag,leg,liber,liver,lide,liter,loc,loco,log,logo,ology\\\n",
    "    ,loqu,locut,luc,lum,lun,lus,lust,lude,macr,macer,magn,main,mal,man,manu,mand,mania,mar\\\n",
    "    ,mari,mer,matri,medi,mega,mem,ment,meso,meta,meter,metr,micro,migra,mill,kilo,milli,min\\\n",
    "    ,mis,mit,miss,mob,mov,mot,mon,mono,mor,mort,morph,multi,nano,nasc,nat,gnant,nai,nat,nasc\\\n",
    "    ,neo,neur,nom,nom,nym,nomen,nomin,non,non,nov,nox,noc,numer,numisma,ob,oc,of,op,oct,oligo\\\n",
    "    ,omni,onym,oper,ortho,over,pac,pair,pare,paleo,pan,para,pat,pass,path,pater,patr,path,pathy\\\n",
    "    ,ped,pod,pedo,pel,puls,pend,pens,pond,per,peri,phage,phan,phas,phen,fan,phant,fant,phe,phil\\\n",
    "    ,phlegma,phobia,phobos,phon,phot,photo,pico,pict,plac,plais,pli,ply,plore,plu,plur,plus,pneuma\\\n",
    "    ,pneumon,pod,poli,poly,pon,pos,pound,pop,port,portion,post,pot,pre,pur,prehendere,prin,prim,\\\n",
    "    prime,pro,proto,psych,punct,pute,quat,quad,quint,penta,quip,quir,quis,quest,quer,re,reg,recti\\\n",
    "    ,retro,ri,ridi,risi,rog,roga,rupt,sacr,sanc,secr,salv,salu,sanct,sat,satis,sci,scio,scientia,\\\n",
    "    scope,scrib,script,se,sect,sec,sed,sess,sid,semi,sen,scen,sent,sens,sept,sequ,secu,sue,serv,\\\n",
    "    sign,signi,simil,simul,sist,sta,stit,soci,sol,solus,solv,solu,solut,somn,soph,spec,spect,spi,\\\n",
    "    spic,sper,sphere,spir,stand,stant,stab,stat,stan,sti,sta,st,stead,strain,strict,string,stige,\\\n",
    "    stru,struct,stroy,stry,sub,suc,suf,sup,sur,sus,sume,sump,super,supra,syn,sym,tact,tang,tag,tig,\\\n",
    "    ting,tain,ten,tent,tin,tect,teg,tele,tem,tempo,ten,tin,tain,tend,tent,tens,tera,term,terr,terra,\\\n",
    "    test,the,theo,therm,thesis,thet,tire,tom,tor,tors,tort,tox,tract,tra,trai,treat,trans,tri,trib,\\\n",
    "    tribute,turbo,typ,ultima,umber,umbraticum,un,uni,vac,vade,vale,vali,valu,veh,vect,ven,vent,ver,\\\n",
    "    veri,verb,verv,vert,vers,vi,vic,vicis,vict,vinc,vid,vis,viv,vita,vivi,voc,voke,vol,volcan,volv\\\n",
    "    ,volt,vol,vor,with,zo\".replace(\" \",\"\").split(\",\")\n",
    "    #print(pre[0].upper())\n",
    "    for i in pre:\n",
    "        if s.startswith(i.upper()):\n",
    "            return 1\n",
    "    return 0\n",
    "def s_has_end(s):\n",
    "    end = \"ee,ese,esque,se,eer,ique,ty,less,ness,ly,ible,able,ion,ic,ical,al,ian,ic,\\\n",
    "    ion,ity,ment,ed,es,er,est,or,ary,ory,ous,cy,ry,ty,al,ure,ute,ble,ar,ly,less,ful,ing,\\\n",
    "    ,inal,tion,sion,osis,oon,sce,\\\n",
    "    que,ette,eer,ee,aire,able,ible,acy,cy,ade,age,al,al,ial,ical,an,ance,ence,ancy,\\\n",
    "    ency,ant,ent,ant,ent,ient,ar,ary,ard,art,ate,ate,ate,ation,cade,drome,ed,ed,en,en,\\\n",
    "    ence,ency,er,ier,er,or,er,or,ery,es,ese,ies,es,ies,ess,est,iest,fold,ful,ful,fy,ia,\\\n",
    "    ian,iatry,ic,ic,ice,ify,ile,ing,ion,ish,ism,ist,ite,ity,ive,ive,ative,itive,ize,less,\\\n",
    "    ly,ment,ness,or,ory,ous,eous,ose,ious,ship,ster,ure,ward,wise,ize,phy,ogy,ity,ion,ic,ical,al\".replace(\" \",\"\").split(\",\")\n",
    "    for i in end:\n",
    "        if s.endswith(i.upper()):\n",
    "            return 1\n",
    "    return 0\n",
    "def rip_number(s):\n",
    "    if s[-1].isdigit():\n",
    "        s = s[:-1]\n",
    "    return s\n",
    "def get_Slice(s,r,vowel,cons):\n",
    "    vocab,phonemes = get_Vocab(s)\n",
    "    #print(vocab)\n",
    "    phonemes = phonemes.split(\" \")\n",
    "    stress_matrix = []\n",
    "    words = vowel+cons\n",
    "    has_pre = 0\n",
    "    has_end = 0\n",
    "    #A SLICE OF FEATURE\n",
    "    #1.音节数量 2.音节组合 5.是否有前缀 6.后缀是否为这些 7.prime\n",
    "    current = []\n",
    "    vowels = [-1,-1,-1,-1]\n",
    "    vowel_pos = [-1,-1,-1,-1,-1,-1,-1,-1]\n",
    "#    vowel_pos2 = [-1,-1,-1,-1]\n",
    "    count_vowel = 0\n",
    "    for i in range(len(phonemes)):\n",
    "        if phonemes[i][-1].isdigit():\n",
    "            stress_matrix.append(phonemes[i][-1])\n",
    "            vowels[count_vowel]=vowel.index(phonemes[i][:-1])\n",
    "            if i>0:\n",
    "                vowel_pos[2*count_vowel] = words.index(rip_number(phonemes[i-1]))\n",
    "            if i+1<len(phonemes):\n",
    "                vowel_pos[1+2*count_vowel] = words.index(rip_number(phonemes[i+1]))\n",
    "            count_vowel+=1\n",
    "            #if -1 in vowel:\n",
    "            #    vowel_r[vowel_r.index(-1)] = vowel.index(phonemes[i][:-1])\n",
    "            #if -1 in vowel_pos:\n",
    "            #    vowel_pos[vowel_pos.index(-1)] = i\n",
    "    if \"2\" in stress_matrix:\n",
    "        prime_pos = stress_matrix.index(\"2\")    \n",
    "    elif \"1\" in stress_matrix:\n",
    "        prime_pos = stress_matrix.index(\"1\")\n",
    "#    has_2 =0\n",
    "#    has_3 =0\n",
    "#    if len(stress_matrix)==2:\n",
    "#        has_2 =1\n",
    "#    if len(stress_matrix)==3:\n",
    "#        has_3 =1\n",
    "    has_pre = s_has_pre(vocab)\n",
    "    has_end = s_has_end(vocab)\n",
    "#    if len(stress_matrix)==3:\n",
    "#        print(prime_pos==0)\n",
    "#    print([len(stress_matrix)]+vowels+[has_pre,has_end,prime_pos])\n",
    "#    if prime_pos ==0:\n",
    "#        print(phonemes)\n",
    "#    r.append([len(stress_matrix),len(phonemes)]+vowels+vowel_pos+vowel_pos2+[vocab,has_pre,has_end,prime_pos+1])\n",
    "    r.append([len(stress_matrix)]+vowels+vowel_pos+[vocab,has_pre,has_end,prime_pos+1])\n",
    "def get_Slice2(s,r,vowel,cons):\n",
    "    vocab,phonemes = get_Vocab(s)\n",
    "    phonemes = phonemes.split(\" \")\n",
    "    words = vowel+cons\n",
    "    #A SLICE OF FEATURE, 1.NUMBER_OF_VOW 2.NUMBER_OF_syllables 3.VOWEL1_pos 4.VOWEL2_pos 5.VOWEL3_pos 6.VOWEL1\n",
    "    #7VOWEL2 8.VOWEL3 9.p_s\n",
    "    current = []\n",
    "    count_vowel = 0\n",
    "    vowels = [-1,-1,-1,-1]\n",
    "    vowel_pos = [-1,-1,-1,-1,-1,-1,-1,-1]\n",
    "#    vowel_pos2 = [-1,-1,-1,-1]\n",
    "\n",
    "    for i in range(len(phonemes)):\n",
    "        if phonemes[i] in vowel: \n",
    "            vowels[count_vowel]=vowel.index(phonemes[i])\n",
    "            if i>0:\n",
    "                vowel_pos[2*count_vowel] = words.index(phonemes[i-1])\n",
    "            if i+1<len(phonemes):\n",
    "                vowel_pos[1+2*count_vowel] = words.index(phonemes[i+1])\n",
    "            count_vowel+=1\n",
    "    has_pre = s_has_pre(vocab)\n",
    "    has_end = s_has_end(vocab)\n",
    "#    has_2 =0\n",
    "#    has_3 =0\n",
    "#    if count_vowel==2:\n",
    "#        has_2 =1\n",
    "#    if count_vowel==3:\n",
    "#        has_3 =1\n",
    "#    print()\n",
    "#    if has_pre:\n",
    "#        print(vocab)\n",
    "#    print([count_vowel,count_vowel==2,count_vowel==3,count_vowel-3,has_pre,has_end])\n",
    "#    r.append([count_vowel,has_2,has_3,count_vowel-3,has_pre,has_end])\n",
    "#    r.append([count_vowel,len(phonemes)]+vowels+vowel_pos+vowel_pos2+[vocab,has_pre,has_end])\n",
    "    r.append([count_vowel]+vowels+vowel_pos+[vocab,has_pre,has_end])\n",
    "\n",
    "\n",
    "def get_Inf(s):\n",
    "    dic = {}\n",
    "    r = []\n",
    "    vowel = \"AA, AE, AH, AO, AW, AY, EH, ER, EY, IH, IY, OW, OY, UH, UW\".replace(\",\",\"\").split()\n",
    "    consonant = \"P, B, CH, D, DH, F, G, HH, JH, K, L, M, N, NG, R, S, SH, T, TH, V, W, Y, Z, ZH\".replace(\",\",\"\").split()\n",
    "    for i in s:\n",
    "        get_Slice(i,r,vowel,consonant)\n",
    "    features_and_label = pd.DataFrame(r)\n",
    "    return features_and_label\n",
    "\n",
    "def get_Inf2(s):\n",
    "    dic = {}\n",
    "    r = []\n",
    "    vowel = \"AA, AE, AH, AO, AW, AY, EH, ER, EY, IH, IY, OW, OY, UH, UW\".replace(\",\",\"\").split()\n",
    "    consonant = \"P, B, CH, D, DH, F, G, HH, JH, K, L, M, N, NG, R, S, SH, T, TH, V, W, Y, Z, ZH\".replace(\",\",\"\").split()\n",
    "    for i in s:\n",
    "        get_Slice2(i,r,vowel,consonant)\n",
    "    features_and_label = pd.DataFrame(r)\n",
    "#    print(features_and_label)\n",
    "    return features_and_label\n",
    "\n",
    "def get_type(s):\n",
    "    types = \"CC,CD,DT,EX,FW,IN,JJ,JJR,JJS,LS,MD,NN,NNS,NNP,NNPS,PDT,POS,PRP,PRP$,RB,RBR,RBS,RP,SYM,TO\\\n",
    "    UH,VB,VBD,VBG,VBN,VBP,VBZ,WDT,WP,WP$,WRB\".split(\",\")\n",
    "    type_list = []\n",
    "#    s = nltk.pos_tag(s)\n",
    "#    print(s)\n",
    "    for i in range (len(s)):\n",
    "        word_type = nltk.pos_tag([s[i].capitalize()])\n",
    "#        print(word_type)\n",
    "        type_list.append(types.index(word_type[0][1]))\n",
    "#    print(type_list)\n",
    "    return type_list\n",
    "\n",
    "\n",
    "################# training #################\n",
    "\n",
    "def train(data, classifier_file):# do not change the heading of the function\n",
    "    #train, test = train_test_split(data, test_size = 0.2)\n",
    "    features_and_label = get_Inf(data)\n",
    "    feature_list = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n",
    "    features_and_label.loc[:,13] = get_type(features_and_label.loc[:,13])\n",
    "    X_train = features_and_label[feature_list]\n",
    "#    print(data[:5])\n",
    "#    print(features_and_label.head())\n",
    "#    print( X_train[10])\n",
    "    y_train = features_and_label[16]\n",
    "#    gnb = GaussianNB()\n",
    "#    gnb.fit(X_train, y_train)\n",
    "#    print(gnb.score(X_train,y_train))\n",
    "#    depth = 0\n",
    "#    for i in range(40,1,-1):\n",
    "#        temp = evaluate(features_and_label,10,i)\n",
    "#        if depth>temp:\n",
    "#            print(i-1)\n",
    "#            depth = i-1\n",
    "#            break\n",
    "#        else:\n",
    "#            depth = temp\n",
    "#    temp = evaluate(features_and_label,10,17)\n",
    "    clf = DecisionTreeClassifier(criterion = \"gini\")\n",
    "#    features_and_label2 = get_Inf(test)\n",
    "    #X_test = features_and_label[feature_list]\n",
    "    #X_test.loc[:,10] = get_type(X_test.loc[:,10])\n",
    "    #y_test = features_and_label2[13]\n",
    "\n",
    "#    clf = DecisionTreeClassifier(criterion = \"gini\")\n",
    "    dtree = clf.fit(X_train, y_train)\n",
    "    \n",
    "    print(dtree.score(X_train,y_train))\n",
    "    output = open('classifier_file', 'wb')\n",
    "    pickle.dump(clf, output)\n",
    "    output.close()\n",
    "    return y_train    \n",
    "\n",
    "################# testing #################\n",
    "\n",
    "def test(data, classifier_file):# do not change the heading of the function\n",
    "    pkl_file = open('classifier_file', 'rb')\n",
    "    dt = pickle.load(pkl_file)\n",
    "    r = []\n",
    "    features_and_label = get_Inf2(data)\n",
    "    features_and_label[13] = get_type(features_and_label[13])\n",
    "    r = dt.predict(features_and_label)\n",
    "    for i in range (len(r)):\n",
    "        if r[i]==0:\n",
    "            r[i]=1\n",
    "    #print(r)\n",
    "    pkl_file.close()\n",
    "    return list(r)\n",
    "\n",
    "def experiment(train, test, features, depth=5):\n",
    "    X_train = train[features]\n",
    "    y_train = train[len(features)]\n",
    "    clf = DecisionTreeClassifier(criterion = \"gini\", max_depth = depth, random_state = 9)\n",
    "    dtree = clf.fit(X_train, y_train)\n",
    "    err_training = dtree.score(X_train,y_train)\n",
    "    X_test = test[features]\n",
    "    y_test = test[len(features)]\n",
    "    err_testing = dtree.score(X_test,y_test)\n",
    "    err_diff = err_training - err_testing\n",
    "    print('{}, {}, {}'.format(err_training, err_testing, err_diff))\n",
    "    return err_training, err_testing\n",
    "        \n",
    "def evaluate(data, repeat_times = 10, depth = 5):\n",
    "    features = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n",
    "    print('features: {}'.format(features))\n",
    "    print('max_depth: {}\\n'.format(depth))\n",
    "    total_err_training = 0\n",
    "    total_err_testing = 0\n",
    "    for i in range(repeat_times):\n",
    "        train_data, test_data = train_test_split(data, test_size = 0.33, random_state = 9 + i)\n",
    "        err_training, err_testing = experiment(train_data, test_data, features, depth)\n",
    "        total_err_training += err_training\n",
    "        total_err_testing += err_testing\n",
    "    return total_err_testing/repeat_times\n",
    "\n",
    "#import helper\n",
    "#import submission\n",
    "\n",
    "#training_data = helper.read_data('./asset/review2.txt')\n",
    "#classifier_path = './asset/classifier2.dat'\n",
    "#train(training_data, classifier_path)\n",
    "#test_data = helper.read_data('./asset/tiny_test.txt')\n",
    "#prediction = test(test_data, classifier_path)\n",
    "#print(prediction)\n",
    "#from sklearn.metrics import f1_score\n",
    "#ground_truth = [1, 1, 2, 1]\n",
    "#print(f1_score(ground_truth, prediction, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prime(s):\n",
    "    dic = {}\n",
    "    r = []\n",
    "    vowel = \"AA, AE, AH, AO, AW, AY, EH, ER, EY, IH, IY, OW, OY, UH, UW\".replace(\",\",\"\").split()\n",
    "    consonant = \"P, B, CH, D, DH, F, G, HH, JH, K, L, M, N, NG, R, S, SH, T, TH, V, W, Y, Z, ZH\".replace(\",\",\"\").split()\n",
    "    for i in s:\n",
    "        get_Slice3(i,r)\n",
    "    #features_and_label = pd.DataFrame(r)\n",
    "    return r\n",
    "def get_Slice3(s,r):\n",
    "    vocab,phonemes = get_Vocab(s)\n",
    "    #print(phonemes)\n",
    "    phonemes = phonemes.split(\" \")\n",
    "    stress_matrix = []\n",
    "    prime_pos = 0\n",
    "    for i in range(len(phonemes)):\n",
    "        if phonemes[i][-1].isdigit():\n",
    "            stress_matrix.append(phonemes[i][-1])\n",
    "    if \"2\" in stress_matrix:\n",
    "        prime_pos = stress_matrix.index(\"2\")+1    \n",
    "    elif \"1\" in stress_matrix:\n",
    "        prime_pos = stress_matrix.index(\"1\")+1\n",
    "    r.append(prime_pos)\n",
    "ground_truth = get_prime(helper.read_data('./asset/training_data.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.998854262145\n",
      "0.657356800273\n"
     ]
    }
   ],
   "source": [
    "training_data = helper.read_data('./asset/review2.txt')\n",
    "classifier_path = './asset/classifier2.dat'\n",
    "#nearly 20% of basedata\n",
    "train(training_data, classifier_path)\n",
    "test_data = helper.read_data('./asset/2.txt')\n",
    "prediction = test(test_data, classifier_path)\n",
    "#print(prediction)\n",
    "from sklearn.metrics import f1_score\n",
    "print(f1_score(ground_truth, prediction, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.998858238948\n",
      "0.807869772819\n"
     ]
    }
   ],
   "source": [
    "training_data = helper.read_data('./asset/review.txt')\n",
    "classifier_path = './asset/classifier2.dat'\n",
    "train(training_data, classifier_path)\n",
    "#nearly 40% of basedata\n",
    "test_data = helper.read_data('./asset/2.txt')\n",
    "prediction = test(test_data, classifier_path)\n",
    "#print(prediction)\n",
    "from sklearn.metrics import f1_score\n",
    "print(f1_score(ground_truth, prediction, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = helper.read_data('./asset/review.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import helper\n",
    "training_data = helper.read_data('./asset/training_data.txt')\n",
    "features_and_label = get_Inf(training_data)\n",
    "feature_list = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n",
    "features_and_label.loc[:,13] = get_type(features_and_label.loc[:,13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>15</td>\n",
       "      <td>34</td>\n",
       "      <td>36</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>29</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>-1</td>\n",
       "      <td>26</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>28</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>15</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16\n",
       "0   2  11   6  -1  -1  24   6  11  18  -1  -1  -1  -1  11   1   1   2\n",
       "1   2   7  14  -1  -1  15  34  36  -1  -1  -1  -1  -1  11   1   1   2\n",
       "2   2   6   9  -1  -1  22  22  22  29  -1  -1  -1  -1  11   0   1   1\n",
       "3   3   2   2   9  -1  26  30  30  25  25  28  -1  -1  27   0   1   1\n",
       "4   4   0  12   2   2  27  27  15  37  37  27  27  30   6   1   1   2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_and_label.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         16       \n",
      "       mean    sum\n",
      "0                 \n",
      "2  1.235273  34117\n",
      "3  1.736749  28474\n",
      "4  2.028901  12145\n"
     ]
    }
   ],
   "source": [
    "print(features_and_label.groupby([0]).agg({16: [np.mean,np.sum]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0  14  15  16\n",
       "2  0   1   1     14313\n",
       "           2      3554\n",
       "   1   1   1      6808\n",
       "           2      2944\n",
       "3  0   1   1      3744\n",
       "           2      3240\n",
       "           3      1101\n",
       "   1   1   1      3176\n",
       "           2      3631\n",
       "           3      1503\n",
       "4  0   1   1       735\n",
       "           2       418\n",
       "           3       649\n",
       "           4        89\n",
       "   1   1   1      1606\n",
       "           2      1128\n",
       "           3      1035\n",
       "           4       326\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(features_and_label.groupby([0,14,15,16]).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0  13  16\n",
       "2  0   1         3\n",
       "   2   1         4\n",
       "       2         1\n",
       "   4   1         6\n",
       "   5   1        81\n",
       "       2        84\n",
       "   6   1       441\n",
       "       2        88\n",
       "   7   1        16\n",
       "       2         1\n",
       "   8   1       100\n",
       "       2        14\n",
       "   10  2         2\n",
       "   11  1     13222\n",
       "       2      4408\n",
       "   12  1      2538\n",
       "       2       986\n",
       "   13  1      2597\n",
       "       2       424\n",
       "   17  1         3\n",
       "       2         2\n",
       "   18  1        17\n",
       "   19  1       254\n",
       "       2        44\n",
       "   23  1         1\n",
       "   25  1       238\n",
       "       2       128\n",
       "   26  1        50\n",
       "       2        18\n",
       "   27  1      1059\n",
       "             ...  \n",
       "4  11  2       699\n",
       "       3       874\n",
       "       4       229\n",
       "   12  1       350\n",
       "       2       249\n",
       "       3       241\n",
       "       4        63\n",
       "   13  1       144\n",
       "       2        48\n",
       "       3       150\n",
       "       4        15\n",
       "   14  2         1\n",
       "   18  1         1\n",
       "   19  1       138\n",
       "       2       123\n",
       "       3         4\n",
       "       4         4\n",
       "   25  1         9\n",
       "       2         5\n",
       "       3         5\n",
       "       4        46\n",
       "   26  1         1\n",
       "   27  1        77\n",
       "       2        91\n",
       "       3       218\n",
       "   28  1        73\n",
       "       2        21\n",
       "       3       118\n",
       "       4        54\n",
       "   34  1         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(features_and_label.groupby([0,13,16]).size())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
